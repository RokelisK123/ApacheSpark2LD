{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49IBBE9JYEJQ",
        "outputId": "1c5d8e84-457d-48a2-ef61-ace8627024df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark sesija\n",
            "Spark sesija inicijuota. Skaitomi duomenys iš: duom_full.txt\n",
            "Įkeliami duomenys iš duom_full.txt...\n",
            "Apdorojami duomenys...\n",
            "Talpinami duomenys atmintyje...\n",
            "Sėkmingai apdorota ir išsaugota 59659 įrašų.\n"
          ]
        }
      ],
      "source": [
        "# Importuojamos bibliotekos\n",
        "from pyspark.sql import SparkSession  # Spark funkcionalumo pagrindas\n",
        "import re  # Reguliariosios išraiškos (naudojama tik dalijimui)\n",
        "import time  # Laiko matavimui (nenaudojama skaičiavimams)\n",
        "\n",
        "# Konfigūracija\n",
        "file_path = \"duom_full.txt\"\n",
        "\n",
        "# Pagalbinė funkcija linijų apdorojimui\n",
        "def parse_line_simple(line_text):\n",
        "    \"\"\"\n",
        "    Konvertuoja eilutę '{{key1=value1}{key2=value2}...}' į žodyną {'key1': 'value1', 'key2': 'value2', ...}\n",
        "\n",
        "    Args:\n",
        "        line_text (str): Viena eilutė iš failo.\n",
        "\n",
        "    Returns:\n",
        "        dict: Žodynas su duomenimis arba None, jei eilutė tuščia ar netinkama.\n",
        "    \"\"\"\n",
        "    record = {}\n",
        "    try:\n",
        "        if not line_text or not line_text.strip():\n",
        "            return None\n",
        "\n",
        "        # Pašalina '{{' ir '}}', padalija į dalis pagal '}{'\n",
        "        content = line_text.strip().strip('{}')\n",
        "        parts = content.split('}{')\n",
        "\n",
        "        # Apdoroja kiekvieną dalį (pvz., 'a=1')\n",
        "        for part in parts:\n",
        "            part_cleaned = part.strip('{}')\n",
        "            key_value = part_cleaned.split('=', 1)\n",
        "\n",
        "            if len(key_value) == 2:\n",
        "                key, value = key_value[0].strip(), key_value[1].strip()\n",
        "                if key:\n",
        "                    record[key] = value\n",
        "\n",
        "        return record if record else None\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# Spark sesijos inicijavimas\n",
        "print(\"Spark sesija\")\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"LogisticsRDDAnalysis_Simple\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "sc.setLogLevel(\"ERROR\")  # Mažiau informacinių pranešimų\n",
        "\n",
        "print(f\"Spark sesija inicijuota. Skaitomi duomenys iš: {file_path}\")\n",
        "\n",
        "# Duomenų įkėlimas ir apdorojimas\n",
        "try:\n",
        "    print(f\"Įkeliami duomenys iš {file_path}...\")\n",
        "    raw_lines_rdd = sc.textFile(file_path)\n",
        "\n",
        "    print(\"Apdorojami duomenys...\")\n",
        "    parsed_rdd_with_none = raw_lines_rdd.map(parse_line_simple)\n",
        "    parsed_rdd = parsed_rdd_with_none.filter(lambda record: record is not None)\n",
        "\n",
        "    print(\"Talpinami duomenys atmintyje...\")\n",
        "    parsed_rdd.cache()\n",
        "\n",
        "    record_count = parsed_rdd.count()\n",
        "    if record_count == 0:\n",
        "        print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "        print(f\"Klaida: Nepavyko apdoroti duomenų iš failo: {file_path}\")\n",
        "        print(f\"Patikrinkite failo kelią ir formatą '{{key=value}}{{key=value}}...'\")\n",
        "        print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "        spark.stop()\n",
        "        exit()\n",
        "    else:\n",
        "        print(f\"Sėkmingai apdorota ir išsaugota {record_count} įrašų.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Klaida skaitant failą '{file_path}': {e}\")\n",
        "    print(\"Patikrinkite, ar failas egzistuoja ir kelias teisingas.\")\n",
        "    spark.stop()\n",
        "    exit()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Užduotis 1\n",
        "\n",
        "print(\"\\n 1 užduotis\")\n",
        "\n",
        "\n",
        "# Apibrėžiame funkciją svoriui išgauti ir derivinam grupę pagal svorio vertę.\n",
        "def derive_group_and_extract_weight(record):\n",
        "    \"\"\"\n",
        "    Gets 'svoris' from a record, converts it to float,\n",
        "    DERIVES the weight group based on the float value,\n",
        "    and returns a list containing a tuple `[(derived_group, weight_float)]`.\n",
        "    Returns an empty list `[]` if 'svoris' key is missing or value is not a valid number.\n",
        "    \"\"\"\n",
        "    weight_str = record.get('svoris')\n",
        "\n",
        "    if weight_str is not None: # Patikrinam, ar egzistuoja raktas „svoris\n",
        "        try:\n",
        "            weight_float = float(weight_str) # konvertuojam stringą į skaičių\n",
        "\n",
        "            if weight_float < 50:\n",
        "                derived_group = '<50'\n",
        "            elif weight_float < 300: # jei ne < 50, tikrinam ar < 300\n",
        "                derived_group = '<300'\n",
        "            else: # galiausiai lieka tik >= 300\n",
        "                derived_group = '>300'\n",
        "\n",
        "\n",
        "            return [(derived_group, weight_float)]\n",
        "\n",
        "        except ValueError:\n",
        "            # Jei 'svoris' negali būti konvertuotas į float (pvz.: tekstas)\n",
        "            return [] # returninam empty listą\n",
        "    else:\n",
        "        # Jei nebuvo rakto „svoris\n",
        "        return [] # irgi grąžinam empty listą\n",
        "\n",
        "# Naudojau `flatMap` funkciją, kad gauti (derived_group, weight) poras.\n",
        "weight_data_rdd = parsed_rdd.flatMap(derive_group_and_extract_weight) #\n",
        "\n",
        "\n",
        "# Apibrėžiu \"zero value\"\n",
        "initial_accumulator = (0.0, 0, float('inf'), float('-inf'))\n",
        "\n",
        "# sequence funkcija\n",
        "def merge_value_into_accumulator(acc, value):\n",
        "    current_sum, current_count, current_min, current_max = acc\n",
        "    new_sum = current_sum + value\n",
        "    new_count = current_count + 1\n",
        "    new_min = min(current_min, value)\n",
        "    new_max = max(current_max, value)\n",
        "    return (new_sum, new_count, new_min, new_max)\n",
        "\n",
        "# combiner funkcija\n",
        "def combine_accumulators(acc1, acc2):\n",
        "    sum1, count1, min1, max1 = acc1\n",
        "    sum2, count2, min2, max2 = acc2\n",
        "    combined_sum = sum1 + sum2\n",
        "    combined_count = count1 + count2\n",
        "    combined_min = min(min1, min2)\n",
        "    combined_max = max(max1, max2)\n",
        "    return (combined_sum, combined_count, combined_min, combined_max)\n",
        "\n",
        "# agregacija\n",
        "aggregated_stats_rdd = weight_data_rdd.aggregateByKey(\n",
        "    initial_accumulator,\n",
        "    merge_value_into_accumulator,\n",
        "    combine_accumulators\n",
        ")\n",
        "\n",
        "# funkcija isvesciai\n",
        "def format_weight_stats(item):\n",
        "    group, stats = item\n",
        "    total_sum, count, min_val, max_val = stats\n",
        "    avg_val = total_sum / count if count > 0 else 0.0\n",
        "    min_val = min_val if min_val != float('inf') else \"N/A\"\n",
        "    max_val = max_val if max_val != float('-inf') else \"N/A\"\n",
        "    return (group, {'min': min_val, 'max': max_val, 'avg': avg_val, 'count': count})\n",
        "\n",
        "# formatavimas\n",
        "formatted_stats_rdd = aggregated_stats_rdd.map(format_weight_stats)\n",
        "\n",
        "# rezultatu surinkimas\n",
        "weight_stats_list = formatted_stats_rdd.collect()\n",
        "\n",
        "\n",
        "# Apibrėžkiame pagalbinę funkciją, kad kiekvienam grupės pavadinimui būtų priskirtas rūšiavimo eilės numeris\n",
        "def get_group_sort_order(group_name):\n",
        "    if group_name == '<50':\n",
        "        return 0  # pirma grupe\n",
        "    elif group_name == '<300':\n",
        "        return 1  # antra grupe\n",
        "    elif group_name == '>300':\n",
        "        return 2  # trecia grupe\n",
        "    else:\n",
        "        return 99 #netiketos/unikalios grupes paciam gale\n",
        "\n",
        "if weight_stats_list:\n",
        "    print(\"Statistika:\")\n",
        "\n",
        "\n",
        "    # Rūšiuojam sąrašą su custom key funkcija\n",
        "    # key funkcija taikoma kiekvienam sąrašo elementui (item[0] yra grupės pavadinimas)\n",
        "    sorted_weight_stats_list = sorted(weight_stats_list, key=lambda item: get_group_sort_order(item[0]))\n",
        "\n",
        "    # iteruojame per listą\n",
        "    for group, stats in sorted_weight_stats_list:\n",
        "        print(f\"  Svorio grupė '{group}':\")\n",
        "        print(f\"    Minimalus svoris: {stats['min']:.2f}\" if isinstance(stats['min'], float) else f\"    Min Weight: {stats['min']}\")\n",
        "        print(f\"    Maksimalus svoris: {stats['max']:.2f}\" if isinstance(stats['max'], float) else f\"    Max Weight: {stats['max']}\")\n",
        "        print(f\"    Vidutiniškas svoris: {stats['avg']:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxzrmpUaYGLS",
        "outputId": "db5528fb-5e89-449d-af32-ae69640fe583"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 1 užduotis\n",
            "Statistika:\n",
            "  Svorio grupė '<50':\n",
            "    Minimalus svoris: 0.00\n",
            "    Maksimalus svoris: 49.95\n",
            "    Vidutiniškas svoris: 5.91\n",
            "  Svorio grupė '<300':\n",
            "    Minimalus svoris: 50.00\n",
            "    Maksimalus svoris: 299.20\n",
            "    Vidutiniškas svoris: 107.82\n",
            "  Svorio grupė '>300':\n",
            "    Minimalus svoris: 300.00\n",
            "    Maksimalus svoris: 6784.00\n",
            "    Vidutiniškas svoris: 748.02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Užduotis 2\n",
        "\n",
        "print(\"\\n 2 užduotis\")\n",
        "\n",
        "\n",
        "# funkcija ištraukimui route, zone, and date.\n",
        "def extract_route_zone_date_info(record):\n",
        "\n",
        "    #Gauna 'marsrutas', 'geografinė zona' ir 'sustojimo duomenys' iš įrašo.\n",
        "    #Grąžina sąrašą, kuriame yra tuple `[(maršrutas, zona, data)]`, jei maršrutas ir zona egzistuoja,\n",
        "    #arba tuščias sąrašas `[]`.\n",
        "\n",
        "    route = record.get('marsrutas')\n",
        "    zone = record.get('geografine zona')\n",
        "    date = record.get('sustojimo data') # jeigu trūksta bus none\n",
        "\n",
        "    if route and zone: # Reikia tik route ir zone užduočiai\n",
        "        return [(route, zone, date)]\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "# `flatMap` ištraukti informacijai\n",
        "# Inputo RDD: Dictionaries\n",
        "# Outputo RDD (`route_zone_date_rdd`): ('102', 'Z1', '2018-01-02'), ('102', 'Z1', None), ...\n",
        "route_zone_date_rdd = parsed_rdd.flatMap(extract_route_zone_date_info)\n",
        "\n",
        "# jeigu rdd per didelis ir letai veikia\n",
        "# route_zone_date_rdd.cache()\n",
        "\n",
        "# Pasirenkam tik maršrutą ir zoną.\n",
        "route_zone_rdd = route_zone_date_rdd.map(lambda x: (x[0], x[1])) # (route, zone)\n",
        "\n",
        "# Surandame visą skaičių UNIQUE maršrutų skaičiuojant procentaliai\n",
        "total_unique_routes_rdd = route_zone_rdd.map(lambda x: x[0]).distinct()\n",
        "total_unique_routes_count = total_unique_routes_rdd.count()\n",
        "\n",
        "# Surandame unique route ir zone poras\n",
        "distinct_route_zone_rdd = route_zone_rdd.distinct()\n",
        "\n",
        "# Groupinam\n",
        "zones_per_route_rdd = distinct_route_zone_rdd.groupByKey()\n",
        "\n",
        "# filtruojam pagal tai, ar unique zones daugiau uz 1\n",
        "multi_zone_routes_grouped_rdd = zones_per_route_rdd.filter(lambda x: len(list(x[1])) > 1)\n",
        "multi_zone_route_ids_rdd = multi_zone_routes_grouped_rdd.map(lambda x: x[0])\n",
        "multi_zone_routes_list = multi_zone_route_ids_rdd.collect()\n",
        "\n",
        "# skaičiuojam procentus\n",
        "percentage_part_a = (len(multi_zone_routes_list) / total_unique_routes_count * 100) if total_unique_routes_count > 0 else 0\n",
        "\n",
        "if multi_zone_routes_list:\n",
        "    print(f\"Maršrutų kiekis: {len(multi_zone_routes_list)} , kurių metu aplankoma daugiau nei vieną geografinė zon (bendrai paėmus).\")\n",
        "    print(f\"  Procentas: {percentage_part_a:.2f}%\\n\")\n",
        "\n",
        "    routes_to_show = sorted(multi_zone_routes_list)\n",
        "    print(f\"  Maršrutai: {routes_to_show[:100]}{'...' if len(routes_to_show) > 100 else ''}\")\n",
        "\n",
        "# isfiltruojam kur datos nėra\n",
        "# Input RDD (`route_zone_date_rdd`): ('102', 'Z1', '2018-01-02'), ('102', 'Z1', None), ...\n",
        "rdd_with_dates = route_zone_date_rdd.filter(lambda x: x[2] is not None) # x[2] is the date\n",
        "\n",
        "# vėl ieškome unique route ir data porų procentam skaičiuoti\n",
        "total_unique_route_date_rdd = rdd_with_dates.map(lambda x: (x[0], x[2])).distinct()\n",
        "total_unique_route_date_count = total_unique_route_date_rdd.count()\n",
        "\n",
        "\n",
        "# Sukuriam key-value porą, kur raktas yra route ir data o value - zona\n",
        "route_date_key_rdd = rdd_with_dates.map(lambda x: ((x[0], x[2]), x[1]))\n",
        "\n",
        "# ieskome situ unique kombinaciju\n",
        "distinct_route_date_zone_rdd = route_date_key_rdd.distinct()\n",
        "zones_per_route_day_rdd = distinct_route_date_zone_rdd.groupByKey()\n",
        "\n",
        "# kaip ir pirmai, filtriuojam kur unique zones skaičius > 1\n",
        "multi_zone_same_day_grouped_rdd = zones_per_route_day_rdd.filter(lambda x: len(list(x[1])) > 1)\n",
        "multi_zone_same_day_keys_rdd = multi_zone_same_day_grouped_rdd.map(lambda x: x[0])\n",
        "multi_zone_same_day_list = multi_zone_same_day_keys_rdd.collect()\n",
        "\n",
        "# skaičiuoajm procentą\n",
        "percentage_part_b = (len(multi_zone_same_day_list) / total_unique_route_date_count * 100) if total_unique_route_date_count > 0 else 0\n",
        "\n",
        "if multi_zone_same_day_list:\n",
        "    print(f\"Maršrutų skaičius {len(multi_zone_same_day_list)} ,kai tą pačią dieną aplankoma daugiau nei vieną zona (iš unikalių porų):\")\n",
        "    print(f\"  Procentas to skaičiaus, kai aplankoma daugiau nei viena zona (iš unikalių porų): {percentage_part_b:.2f}%\\n\")\n",
        "\n",
        "    # sortinam\n",
        "    sorted_list = sorted(multi_zone_same_day_list, key=lambda item: (str(item[0]), item[1]))\n",
        "    print(\"  Pavyzdžiai: \")\n",
        "    for route, date in sorted_list[:10]:\n",
        "        print(f\"    Maršrutas: {route}, Data (metai, mėnesis, diena): {date}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyXXHiUyYfvY",
        "outputId": "eab49f32-d2d4-4765-9805-9266e3d60324"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 2 užduotis\n",
            "Maršrutų kiekis: 339 , kurių metu aplankoma daugiau nei vieną geografinė zon (bendrai paėmus).\n",
            "  Procentas: 80.33%\n",
            "\n",
            "  Maršrutai: ['103', '107', '109', '110', '111', '112', '113', '114', '116', '117', '119', '127', '128', '131', '137', '138', '140', '141', '142', '143', '144', '145', '146', '148', '150', '151', '152', '153', '154', '156', '157', '160', '161', '163', '164', '165', '166', '167', '170', '171', '172', '173', '174', '175', '176', '179', '203', '204', '205', '207', '208', '209', '210', '211', '212', '213', '214', '216', '217', '218', '219', '220', '221', '222', '223', '224', '227', '228', '229', '230', '232', '234', '236', '238', '240', '241', '243', '244', '245', '246', '247', '248', '250', '251', '252', '253', '254', '259', '260', '261', '262', '263', '267', '268', '269', '280', '281', '283', '284', '285']...\n",
            "Maršrutų skaičius 3012 ,kai tą pačią dieną aplankoma daugiau nei vieną zona (iš unikalių porų):\n",
            "  Procentas to skaičiaus, kai aplankoma daugiau nei viena zona (iš unikalių porų): 39.43%\n",
            "\n",
            "  Pavyzdžiai: \n",
            "    Maršrutas: 103, Data (metai, mėnesis, diena): 2018-01-18\n",
            "    Maršrutas: 107, Data (metai, mėnesis, diena): 2018-01-16\n",
            "    Maršrutas: 109, Data (metai, mėnesis, diena): 2018-01-18\n",
            "    Maršrutas: 110, Data (metai, mėnesis, diena): 2018-01-23\n",
            "    Maršrutas: 111, Data (metai, mėnesis, diena): 2018-01-04\n",
            "    Maršrutas: 112, Data (metai, mėnesis, diena): 2018-01-24\n",
            "    Maršrutas: 113, Data (metai, mėnesis, diena): 2018-01-18\n",
            "    Maršrutas: 114, Data (metai, mėnesis, diena): 2018-01-05\n",
            "    Maršrutas: 114, Data (metai, mėnesis, diena): 2018-01-12\n",
            "    Maršrutas: 116, Data (metai, mėnesis, diena): 2018-01-02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Užduotis 3\n",
        "\n",
        "print(\"\\n 3 užduotis\")\n",
        "\n",
        "\n",
        "# Apibrėžiame funkciją, skirtą zonai, savaitės dienai, siuntoms ir klientams išskirti.\n",
        "def extract_zone_weekday_counts(record):\n",
        "\n",
        "    #Gauna 'geografine zona', 'sustojimo savaites diena', 'siuntu skaicius',\n",
        "    #ir 'Sustojimo klientu skaicius'.\n",
        "    #Konvertuoja counts į int.\n",
        "    #Grąžina sąrašą `[ ((zona, savaitės diena), (siuntos, klientai)) ]`, jei pavyko,\n",
        "    #arba `[]` priešingu atveju. Raktas yra tuple (zona, savaitės diena), reikšmė - tuple (siuntos, klientai).\n",
        "\n",
        "    zone = record.get('geografine zona')\n",
        "    weekday_str = record.get('sustojimo savaites diena')\n",
        "    shipments_str = record.get('siuntu skaicius')\n",
        "    customers_str = record.get('Sustojimo klientu skaicius')\n",
        "\n",
        "    # ziurime ar yra visi laukai\n",
        "    if zone and weekday_str and shipments_str and customers_str:\n",
        "        try:\n",
        "            # konvertuojam skaicius is stringu i integerius\n",
        "            weekday = int(weekday_str)\n",
        "            shipments = int(shipments_str)\n",
        "            customers = int(customers_str)\n",
        "\n",
        "            # Key: (zone, weekday)  Value: (shipments, customers)\n",
        "            return [((zone, weekday), (shipments, customers))]\n",
        "        except ValueError:\n",
        "            # Jei nepavyksta konvertuoti kurio nors skaičiaus\n",
        "            return []\n",
        "    else:\n",
        "        # jeigu truksta kokio rakto\n",
        "        return []\n",
        "\n",
        "# `flatMap`\n",
        "# Input RDD: Dictionaries\n",
        "# Output RDD (`counts_data_rdd`): ( ('Z1', 2), (1, 1) ), ( ('Z1', 2), (4, 1) ), ...\n",
        "counts_data_rdd = parsed_rdd.flatMap(extract_zone_weekday_counts)\n",
        "\n",
        "# Naudokite `reduceByKey`, kad susumuotumėte siuntas ir klientus pagal kiekvieną unikalų raktą (zona, savaitės diena).\n",
        "# `reduceByKey` yra transformacija, kuri sujungia to paties rakto reikšmes naudodama asociatyviąją funkciją.\n",
        "# Čia funkcija priima du reikšmių rinkinius `a = (shipments1, customers1)` and `b = (shipments2, customers2)`\n",
        "# ir grąžina naują tuple `(shipments1+shipments2, customers1+customers2)`.\n",
        "\n",
        "# Input RDD: ( ('Z1', 2), (1, 1) ), ( ('Z1', 2), (4, 1) ), ( ('Z1', 3), (2, 1) ), ...\n",
        "# Output RDD (`aggregated_counts_rdd`): ( ('Z1', 2), (5, 2) ), ( ('Z1', 3), (2, 1) ), ...\n",
        "aggregated_counts_rdd = counts_data_rdd.reduceByKey(\n",
        "    lambda tuple_a, tuple_b: (tuple_a[0] + tuple_b[0], tuple_a[1] + tuple_b[1])\n",
        "    # tuple_a[0] yra siuntos iš pirmo tuple, tuple_b[0] yra siuntos iš antro tuple\n",
        "    # tuple_a[1] yra klientai iš pirmojo tuple, tuple_b[1] yra klientai iš antrojo tuple\n",
        ")\n",
        "\n",
        "#sortinam\n",
        "sorted_counts_rdd = aggregated_counts_rdd.sortByKey(ascending=True)\n",
        "\n",
        "# sucollectiname rezultatus\n",
        "final_counts_list = sorted_counts_rdd.collect()\n",
        "\n",
        "# printinam lentelės formate\n",
        "if final_counts_list:\n",
        "    print(\"Geogr. zona   Savaitės d.  Viso siuntų    Viso klientų\")\n",
        "\n",
        "    # iteracija\n",
        "    for (zone, weekday), (shipments, customers) in final_counts_list:\n",
        "        print(f\"{zone:<12}  {weekday:<11}  {shipments:<13}  {customers:<16} \")\n",
        "\n",
        "\n",
        "# uzbaigiame spark sesiją\n",
        "spark.stop()\n",
        "print(\"baigta.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Gd77vKNYkWb",
        "outputId": "48aec32d-b181-41d6-b3c6-098e65f48fb9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 3 užduotis\n",
            "Geogr. zona   Savaitės d.  Viso siuntų    Viso klientų\n",
            "Z1            1            14092          6867             \n",
            "Z1            2            21024          9979             \n",
            "Z1            3            20494          10106            \n",
            "Z1            4            16775          8110             \n",
            "Z1            5            14653          7616             \n",
            "Z1            6            302            127              \n",
            "Z2            1            2910           1927             \n",
            "Z2            2            4933           3045             \n",
            "Z2            3            5335           3087             \n",
            "Z2            4            3603           2234             \n",
            "Z2            5            4073           2340             \n",
            "Z2            6            23             11               \n",
            "Z3            1            2896           1831             \n",
            "Z3            2            4206           2874             \n",
            "Z3            3            4531           2914             \n",
            "Z3            4            3036           2088             \n",
            "Z3            5            2865           1958             \n",
            "Z3            6            7              5                \n",
            "baigta.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zKS3ahxNZGwl"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jopqijcxZlpg"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}